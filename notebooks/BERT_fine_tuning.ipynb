{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-fine-tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e3UwdnjPnKfR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load SQuAD dataset**"
      ],
      "metadata": {
        "id": "xHZqnWPI0gHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "  train_path = tf.keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
        "  eval_path = tf.keras.utils.get_file(\"eval.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\")\n",
        "  with open(train_path) as f: raw_train_data = json.load(f)\n",
        "  with open(eval_path) as f: raw_eval_data = json.load(f)\n",
        "  return raw_train_data,raw_eval_data"
      ],
      "metadata": {
        "id": "hSxyfPUen6rp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_data, raw_eval_data = load_dataset()"
      ],
      "metadata": {
        "id": "yY24OQzm0wgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22f0fa5-1d2f-4c4b-f6a6-34a84ebc445c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "30294016/30288272 [==============================] - 0s 0us/step\n",
            "30302208/30288272 [==============================] - 0s 0us/step\n",
            "Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "4857856/4854279 [==============================] - 0s 0us/step\n",
            "4866048/4854279 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load BERT model**\n",
        "\n",
        "**Loading of BERT layer from tensorflow hub, and generating the pooled output, sequence output and vocab file from bert layer for question-answering task**"
      ],
      "metadata": {
        "id": "cjuKgAuR1G2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "def load_BERT(url,max_seq_length,trainable=True):\n",
        "  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
        "  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
        "  segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='segment_ids')\n",
        "\n",
        "  bert_layer = hub.KerasLayer(url, trainable=True)\n",
        "\n",
        "  # pooled output has shape (batch size, embedding dim) which is an embedding of the [CLS] token and represents entire sequence\n",
        "  # sequence output has shape (batch size, max sequence length, embedding dim) which has representation for each token \n",
        "\n",
        "  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "  vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
        "  to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "\n",
        "  return vocab_file,bert_layer,pooled_output,sequence_output,input_word_ids, input_mask, segment_ids"
      ],
      "metadata": {
        "id": "fwR7oMyr04af"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
        "max_seq_length = 399\n",
        "\n",
        "vocab_file,bert_layer,pooled_output,sequence_output,input_word_ids, input_mask, segment_ids = load_BERT(url,max_seq_length,True)"
      ],
      "metadata": {
        "id": "Wbjm9JaT5zbl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating the fine tuned BERT model**"
      ],
      "metadata": {
        "id": "7Rx_assd-LjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers"
      ],
      "metadata": {
        "id": "Ep9YpGwF_dEJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelParams:\n",
        "  def __init__(self,learning_rate=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9):\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
        "    self.learning_rate = learning_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.epsilon = epsilon"
      ],
      "metadata": {
        "id": "2ciXtluA-Prr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = ModelParams()"
      ],
      "metadata": {
        "id": "PBiykq4__ND7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_FineTunedBERT():\n",
        "\n",
        "  # start and end logits\n",
        "  start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
        "  start_logits = layers.Flatten()(start_logits)\n",
        "  end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
        "  end_logits = layers.Flatten()(end_logits)\n",
        "\n",
        "  start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "  end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "  model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[start_probs, end_probs])\n",
        "  loss = params.loss\n",
        "  optimizer = params.optimizer\n",
        "  model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Y2eAZigs_bny"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_FineTunedBERT()"
      ],
      "metadata": {
        "id": "RNMMlaJ19vjZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJhE8lQVAmEK",
        "outputId": "c5987291-7923-4efb-eeb8-b8ca4c058b92"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_word_ids (InputLayer)    [(None, 399)]        0           []                               \n",
            "                                                                                                  \n",
            " input_mask (InputLayer)        [(None, 399)]        0           []                               \n",
            "                                                                                                  \n",
            " segment_ids (InputLayer)       [(None, 399)]        0           []                               \n",
            "                                                                                                  \n",
            " keras_layer (KerasLayer)       [(None, 768),        109482241   ['input_word_ids[0][0]',         \n",
            "                                 (None, 399, 768)]                'input_mask[0][0]',             \n",
            "                                                                  'segment_ids[0][0]']            \n",
            "                                                                                                  \n",
            " start_logit (Dense)            (None, 399, 1)       768         ['keras_layer[0][1]']            \n",
            "                                                                                                  \n",
            " end_logit (Dense)              (None, 399, 1)       768         ['keras_layer[0][1]']            \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 399)          0           ['start_logit[0][0]']            \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 399)          0           ['end_logit[0][0]']              \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 399)          0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 399)          0           ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,777\n",
            "Trainable params: 109,483,776\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load BERT tokenizer**"
      ],
      "metadata": {
        "id": "eG9M5AK2BdT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zObYN6VeByHk",
        "outputId": "1879912a-e3a4-476d-af5d-1fca41b325fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 30.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 67.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "def load_tokenizer(vocab_file):\n",
        "  tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "mYQOzZnCAvO5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = load_tokenizer(vocab_file)"
      ],
      "metadata": {
        "id": "_LpQ1XUOB3z4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SQuAD JSON data format**\n",
        "* #### {'data' : {'title' : '',['paragraphs' : [{'context' : '', 'qas' : [{'answers' : [{'answer_start' : '','text' : ''}],'id' : '', 'question' : ''},...]},...],...]}}"
      ],
      "metadata": {
        "id": "_7c7zZ7VCUXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# format:\n",
        "# {'data' : {'title' : '',['paragraphs' : [{'context' : '', 'qas' : [{'answers' : [{'answer_start' : '','text' : ''}],'id' : '', 'question' : ''},...]},...],...]}}"
      ],
      "metadata": {
        "id": "24vxlTUNBNfx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating the training sample prototype**"
      ],
      "metadata": {
        "id": "KWZtO3gWDQeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sample:\n",
        "  def __init__(self, question, context, start_char_idx=None, answer_text=None, all_answers=None):\n",
        "    self.question = question\n",
        "    self.context = context\n",
        "    self.start_char_idx = start_char_idx\n",
        "    self.end_char_idx = -1\n",
        "    self.answer_text = answer_text\n",
        "    self.skip = False\n",
        "    self.start_token_idx = -1\n",
        "    self.end_token_idx = -1\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.padding_length = 10\n",
        "    self.all_answers = all_answers\n",
        "\n",
        "  def get_tokens(self):\n",
        "    context = \" \".join(str(self.context).split())\n",
        "    question = \" \".join(str(self.question).split())\n",
        "\n",
        "    tokenized_context = tokenizer.encode(context)\n",
        "    tokenized_question = tokenizer.encode(question)\n",
        "\n",
        "    return (context,question),(tokenized_context,tokenized_question)\n",
        "\n",
        "  def get_ids(self,tokenized_context,tokenized_question):\n",
        "    input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
        "    seg_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
        "    mask = [1] * len(input_ids)\n",
        "    self.padding_length = self.max_seq_length - len(input_ids)\n",
        "    return (input_ids,seg_ids,mask)\n",
        "\n",
        "  def preprocess(self):\n",
        "\n",
        "    # getting the tokenized text\n",
        "    (context,question),(tokenized_context,tokenized_question) = self.get_tokens()\n",
        "\n",
        "    if self.answer_text is not None:\n",
        "      answer = \" \".join(str(self.answer_text).split())\n",
        "\n",
        "      # calculating end character index\n",
        "      self.end_char_idx = self.start_char_idx + len(answer)\n",
        "      if self.end_char_idx >= len(context):\n",
        "          self.skip = True\n",
        "          return\n",
        "    \n",
        "      is_char_in_ans = [0] * len(context)\n",
        "      for idx in range(self.start_char_idx, self.end_char_idx):\n",
        "          is_char_in_ans[idx] = 1\n",
        "      ans_token_idx = []\n",
        "\n",
        "      # finding the relevant tokens present in the answer\n",
        "      for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
        "          if sum(is_char_in_ans[start:end]) > 0:\n",
        "              ans_token_idx.append(idx)\n",
        "      if len(ans_token_idx) == 0:\n",
        "          self.skip = True\n",
        "          return\n",
        "\n",
        "      self.start_token_idx = ans_token_idx[0]\n",
        "      self.end_token_idx = ans_token_idx[-1]\n",
        "\n",
        "    # getting the ids necessary for BERT input\n",
        "    (input_ids,seg_ids,mask) = self.get_ids(tokenized_context,tokenized_question)\n",
        "\n",
        "    # adding necessary padding \n",
        "    if self.padding_length > 0:\n",
        "        input_ids = input_ids + ([0] * self.padding_length)\n",
        "        mask = mask + ([0] * self.padding_length)\n",
        "        seg_ids = seg_ids + ([0] * self.padding_length)\n",
        "    elif self.padding_length < 0:\n",
        "        self.skip = True\n",
        "        return\n",
        "\n",
        "    self.input_word_ids = input_ids\n",
        "    self.segment_ids = seg_ids\n",
        "    self.input_mask = mask\n",
        "    self.context_token_to_char = tokenized_context.offsets"
      ],
      "metadata": {
        "id": "1GyUKZoeA0Bn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating the training and testing examples**"
      ],
      "metadata": {
        "id": "onv57mWmSJMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_examples(data):\n",
        "    examples = []\n",
        "    for item in data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qas in para[\"qas\"]:\n",
        "                question = qas[\"question\"]\n",
        "                if \"answers\" in qas:\n",
        "                    answer_text = qas[\"answers\"][0][\"text\"]\n",
        "                    start_char_idx = qas[\"answers\"][0][\"answer_start\"]\n",
        "                    all_answers = [_[\"text\"] for _ in qas[\"answers\"]]\n",
        "                    sample = Sample(question, context, start_char_idx, answer_text,all_answers)\n",
        "                else:\n",
        "                    sample = Sample(question, context)\n",
        "\n",
        "                # preprocess each sample\n",
        "                sample.preprocess()\n",
        "                examples.append(sample)\n",
        "    return examples"
      ],
      "metadata": {
        "id": "3WwLjZRTSF3Y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating the data and target pairs**"
      ],
      "metadata": {
        "id": "hSX-RndqS00b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_target_pairs(examples):\n",
        "    dataset_dict = {\n",
        "        \"input_word_ids\": [],\n",
        "        \"segment_ids\": [],\n",
        "        \"input_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in examples:\n",
        "        if item.skip == False:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "    x = [dataset_dict[\"input_word_ids\"],\n",
        "         dataset_dict[\"input_mask\"],\n",
        "         dataset_dict[\"segment_ids\"]]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "okfYiBXaSxbz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = create_examples(raw_train_data)\n",
        "x_train, y_train = create_data_target_pairs(train_examples)\n",
        "\n",
        "test_examples = create_examples(raw_eval_data)\n",
        "x_test, y_test = create_data_target_pairs(test_examples)"
      ],
      "metadata": {
        "id": "UK0vNdfSTPZL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of training examples created : \",np.array(x_train).shape,\" , \",np.array(y_train).shape)\n",
        "print(\"Shape of training examples created : \",np.array(x_test).shape,\" , \",np.array(y_test).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgqeQMB9UDNj",
        "outputId": "aad5851d-520a-4a85-8544-be9b222a457e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training examples created :  (3, 86299, 399)  ,  (2, 86299)\n",
            "Shape of training examples created :  (3, 10349, 399)  ,  (2, 10349)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training the fine tuned BERT model**"
      ],
      "metadata": {
        "id": "ZT3jYl_wxApg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom callback**"
      ],
      "metadata": {
        "id": "89Iw-YwnyZtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re,string\n",
        "\n",
        "class ValidationCallback(keras.callbacks.Callback):\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        # convert to lower case\n",
        "        text = text.lower()\n",
        "        # remove redundant whitespaces\n",
        "        text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
        "        # remove articles\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        text = re.sub(regex, \" \", text)\n",
        "        text = \" \".join(text.split())\n",
        "        return text\n",
        "\n",
        "    def __init__(self, x_eval, y_eval):\n",
        "        self.x_eval = x_eval\n",
        "        self.y_eval = y_eval\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # get the offsets of the first and last tokens of predicted answers\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
        "        count = 0\n",
        "        valid_test_examples = [_ for _ in test_examples if _.skip == False]\n",
        "        # for every pair of offsets\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "            # take the required Sample object with the ground-truth answers in it\n",
        "            example = valid_test_examples[idx]\n",
        "            # use offsets to get back the span of text corresponding to\n",
        "            # our predicted first and last tokens\n",
        "            offsets = example.context_token_to_char\n",
        "            start = np.argmax(start)\n",
        "            end = np.argmax(end)\n",
        "            if start >= len(offsets):\n",
        "                continue\n",
        "            pred_char_start = offsets[start][0]\n",
        "            if end < len(offsets):\n",
        "                pred_char_end = offsets[end][1]\n",
        "                pred_ans = example.context[pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                pred_ans = example.context[pred_char_start:]\n",
        "            normalized_pred_ans = self.normalize_text(pred_ans)\n",
        "            # clean the real answers\n",
        "            normalized_true_ans = [self.normalize_text(_) for _ in example.all_answers]\n",
        "            # check if the predicted answer is in an array of the ground-truth answers\n",
        "            if normalized_pred_ans in normalized_true_ans:\n",
        "                count += 1\n",
        "        acc = count / len(self.y_eval[0])\n",
        "        print(f\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\")"
      ],
      "metadata": {
        "id": "caqfriQ1yKtF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=2, batch_size=8, callbacks=[ValidationCallback(x_test, y_test)])"
      ],
      "metadata": {
        "id": "0DZnGC4JVBWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec00c1e-eee7-4ae0-97e0-30a497686b60"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "10788/10788 [==============================] - ETA: 0s - loss: 2.6514 - activation_loss: 1.3846 - activation_1_loss: 1.2669\n",
            "epoch=1, exact match score=0.77\n",
            "10788/10788 [==============================] - 9165s 848ms/step - loss: 2.6514 - activation_loss: 1.3846 - activation_1_loss: 1.2669\n",
            "Epoch 2/2\n",
            "10788/10788 [==============================] - ETA: 0s - loss: 1.6922 - activation_loss: 0.8986 - activation_1_loss: 0.7936\n",
            "epoch=2, exact match score=0.79\n",
            "10788/10788 [==============================] - 9145s 848ms/step - loss: 1.6922 - activation_loss: 0.8986 - activation_1_loss: 0.7936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"./weights.h5\")"
      ],
      "metadata": {
        "id": "OgI60tkxWtxN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Testing on unknown data**"
      ],
      "metadata": {
        "id": "TeO_5tMP0QNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_FineTunedBERT()\n",
        "model.load_weights('weights.h5')"
      ],
      "metadata": {
        "id": "aG4dvqBI4auh"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predicted_answers(data):\n",
        "  test_samples = create_examples(data)\n",
        "  x_test, _ = create_data_target_pairs(test_samples)\n",
        "  pred_start, pred_end = model.predict(x_test)\n",
        "  answers = []\n",
        "  questions = []\n",
        "  for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "      test_sample = test_samples[idx]\n",
        "      offsets = test_sample.context_token_to_char\n",
        "      start = np.argmax(start)\n",
        "      end = np.argmax(end)\n",
        "      pred_ans = None\n",
        "      if start >= len(offsets):\n",
        "          continue\n",
        "      pred_char_start = offsets[start][0]\n",
        "      if end < len(offsets):\n",
        "          pred_ans = test_sample.context[pred_char_start:offsets[end][1]]\n",
        "      else:\n",
        "          pred_ans = test_sample.context[pred_char_start:]\n",
        "      questions.append(test_sample.question)\n",
        "      answers.append(pred_ans)\n",
        "  return questions,answers"
      ],
      "metadata": {
        "id": "wT1GKhi91mvp"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_question_answers(questions,answers):\n",
        "  for question,answer in zip(questions,answers):\n",
        "    print(\"Q: \" + question)\n",
        "    print(\"A: \" + answer)"
      ],
      "metadata": {
        "id": "PaQSlisT15h6"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c1 = '''Shah Rukh Khan (pronounced [ˈʃɑːɦɾʊx xɑːn]; born 2 November 1965), also known by the initialism SRK, is an Indian actor, film producer, and television personality who works in Hindi films. Referred to in the media as the \"Baadshah of Bollywood\" (in reference to his 1999 film Baadshah), \"King of Bollywood\" and \"King Khan\", he has appeared in more than 80 films, and earned numerous accolades, including 14 Filmfare Awards. The Government of India has awarded him the Padma Shri, and the Government of France has awarded him the Ordre des Arts et des Lettres and the Legion of Honour. Khan has a significant following in Asia and the Indian diaspora worldwide. In terms of audience size and income, he has been described as one of the most successful film stars in the world.'''"
      ],
      "metadata": {
        "id": "eJXGuN4uywln"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = {\"data\":\n",
        "    [\n",
        "        {\"title\": \"Shah Rukh Khan\",\n",
        "         \"paragraphs\": [\n",
        "             {\n",
        "                 \"context\": c1,\n",
        "                 \"qas\": [\n",
        "                     {\"question\": \"What name is Shah Rukh Khan refered to the media as?\",\n",
        "                      \"id\": \"Q1\"\n",
        "                      },\n",
        "                     {\"question\": \"How many films did he appear in?\",\n",
        "                      \"id\": \"Q2\"\n",
        "                      }\n",
        "                 ]}]}]}\n",
        "\n",
        "questions,answers = get_predicted_answers(data1)\n",
        "display_question_answers(questions,answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYI6Y0sEzNju",
        "outputId": "009a407c-d7eb-467e-83e5-6b4322079d52"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What name is Shah Rukh Khan refered to the media as?\n",
            "A: Baadshah of Bollywood\n",
            "Q: How many films did he appear in?\n",
            "A: more than 80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c2 = '''As of 2015, Khan is co-chairman of the motion picture production company Red Chillies Entertainment and its subsidiaries and is the co-owner of the Indian Premier League cricket team Kolkata Knight Riders and the Caribbean Premier League team Trinbago Knight Riders. He is a frequent television presenter and stage show performer. The media often label him as \"Brand SRK\" because of his many endorsement and entrepreneurship ventures. Khan's philanthropic endeavours have provided health care and disaster relief, and he was honoured with UNESCO's Pyramide con Marni award in 2011 for his support of children's education and the World Economic Forum's Crystal Award in 2018 for his leadership in championing women's and children's rights in India. He regularly features in listings of the most influential people in Indian culture, and in 2008, Newsweek named him one of their fifty most powerful people in the world.'''"
      ],
      "metadata": {
        "id": "_VNF0Fu-zlWY"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = {\"data\":\n",
        "    [\n",
        "        {\"title\": \"Shah Rukh Khan\",\n",
        "         \"paragraphs\": [\n",
        "             {\n",
        "                 \"context\": c2,\n",
        "                 \"qas\": [\n",
        "                     {\"question\": \"Shah Rukh Khan is the chairman of which production company?\",\n",
        "                      \"id\": \"Q1\"\n",
        "                      },\n",
        "                     {\"question\": \"He was awarded which award by UNESCO?\",\n",
        "                      \"id\": \"Q2\"\n",
        "                      }\n",
        "                 ]}]}]}\n",
        "\n",
        "questions,answers = get_predicted_answers(data2)\n",
        "display_question_answers(questions,answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpzdJ0v31jln",
        "outputId": "d690eb5b-c170-45b1-8151-362d3d6cbe5a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Shah Rukh Khan is the chairman of which production company?\n",
            "A: Red Chillies Entertainment\n",
            "Q: He was awarded which award by UNESCO?\n",
            "A: Pyramide con Marni award\n"
          ]
        }
      ]
    }
  ]
}